{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60664c42-a7e6-4a3d-b485-120924c13054",
   "metadata": {},
   "source": [
    "# Assignment 1 \n",
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efb494f9-7e59-4ac2-b394-42bd81a70b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from elasticsearch7 import Elasticsearch\n",
    "from elasticsearch7.client import IndicesClient\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee34caa7-56a4-44c1-8ee1-c516fa0055f5",
   "metadata": {},
   "source": [
    "ps = PorterStemmer()\n",
    "stoplist = '../../IR_data/AP_DATA/stoplist.txt'\n",
    "with open(stoplist, 'r') as f:\n",
    "    stop_words = set(f.read().split())\n",
    "def stem_text_and_remove_stopwords(tokens):\n",
    "    processed_tokens = []\n",
    "    for word in tokens: \n",
    "        w = word.strip()\n",
    "        w = ps.stem(w)\n",
    "        if w.lower() not in stop_words:\n",
    "            processed_tokens.append(w)\n",
    "    return ' '.join(processed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30fbf0f1-bdec-4261-a06c-13811143acad",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def stem_text_and_remove_stopwords(tokens):\n",
    "    processed_tokens = []\n",
    "    for word in tokens: \n",
    "        w = ps.stem(word)\n",
    "        w = w.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation))).strip()\n",
    "        if w!='' and w.lower() not in stop_words and w not in processed_tokens:\n",
    "            processed_tokens.append(w)\n",
    "    return ' '.join(processed_tokens)\n",
    "def process_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    processed_text = stem_text_and_remove_stopwords(tokens)\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "992812ac-b578-4334-95d1-6dd6bfc99bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84678\n"
     ]
    }
   ],
   "source": [
    "text_map = defaultdict(str)\n",
    "folder = '../IR_data/AP_DATA/ap89_collection'\n",
    "count = 0\n",
    "for filename in os.listdir(folder):\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    with open(file_path, 'rb') as f:\n",
    "        content = f.read().decode('iso-8859-1')\n",
    "    doc_regex = r'<DOC>(.*?)</DOC>'\n",
    "    for doc in re.findall(doc_regex, content, re.S):\n",
    "        docno = re.search(r'<DOCNO>(.*?)</DOCNO>', doc).group(1).strip()      \n",
    "        for each in re.findall(r'<TEXT>(.*?)</TEXT>', doc, re.S):    \n",
    "            text_map[docno]+= ' ' + process_text(each)\n",
    "                \n",
    "print(len(text_map))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959a93ae-4908-44d5-b6ac-b1413783d53d",
   "metadata": {},
   "source": [
    "### ElasticSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "590560b5-b2e7-411c-8963-fba21c3fadb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "es = Elasticsearch('http://localhost:9200/', timeout=60)\n",
    "print(es.ping())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f8336f3-984c-4377-9e4b-5b2f5e242e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = 'ap89_collection0'\n",
    "\n",
    "configurations = {\n",
    "    \"settings\" : {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 1,\n",
    "        \"analysis\": {\n",
    "            \"filter\": {\n",
    "                \"english_stop\": {\n",
    "                    \"type\": \"stop\",\n",
    "                    \"stopwords_path\": \"my_stoplist.txt\"\n",
    "                }\n",
    "            },\n",
    "            \"analyzer\": {\n",
    "                \"stopped\": {\n",
    "                    \"type\": \"custom\",\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\",\n",
    "                        \"english_stop\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "      }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"content\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fielddata\": True,\n",
    "                \"analyzer\": \"stopped\",\n",
    "                \"index_options\": \"positions\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bca3c9-a34a-48b5-8bc7-388fa11d70f9",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f38d767-4e8f-466c-99b1-95b59eb8db34",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_stop_words = stop_words.union(('document', 'noncommunist', 'locat', 'least', 'countri', 'second', 'unsubstanti', 'worldwid', 'exist', \n",
    "                               'product', 'preliminari', 'perpetr', 'aid', 'success', 'predict', 'describ', 'identifi', 'make', 'undesir',\n",
    "                               'level', 'determin', 'perform', 'platform', 'someth', 'side', 'effort', 'standard', 'motiv',\n",
    "                               'controversi', 'measur', 'tent', 'sign', 'individu', 'develop', 'nation', 'pend',\n",
    "                               'includ', 'result', 'anticip', 'support', 'ani', 'ha', 'directli', 'border' ,'area', 'base',\n",
    "                              'affair', 'ongo', 'method', 'sinc', 'system', 'candid', 'specifi', 'advanc', 'polit', 'attempt', 'asset'\n",
    "                              , 'organ','u s'))\n",
    "def query_stem_text_and_remove_stopwords(tokens):\n",
    "    processed_tokens = []\n",
    "    for word in tokens: \n",
    "        w = ps.stem(word)\n",
    "        w = w.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation))).strip()\n",
    "        if w!='' and w.lower() not in query_stop_words and w not in processed_tokens:\n",
    "            processed_tokens.append(w)\n",
    "    return ' '.join(processed_tokens)\n",
    "    \n",
    "def process_query(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    processed_tokens = query_stem_text_and_remove_stopwords(tokens)\n",
    "    return processed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78007b20-1a51-4fc7-9ff1-2129ca036542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85 discuss alleg taken corrupt public offici government jurisdict\n",
      "59 report type weather event caus one fatal\n",
      "56 prime lend rate report actual move\n",
      "71 report incurs land air water one militari forc guerrilla group\n",
      "64 report event hostage tak\n",
      "62 report militari coup d etat either\n",
      "93 must rifl associ nra\n",
      "99 iran contra\n",
      "58 rail strike report\n",
      "77 report poach use certain type wildlif\n",
      "54 cite contract agreement reserv launch commerci satellit\n",
      "87 report current crimin action offic fail financi institut\n",
      "94 must crime comput\n",
      "100 non communist industri state regul transfer high tech good dual us technolog\n",
      "89 must invest opec member state downstream oper\n",
      "61 discuss role israel iran contra\n",
      "95 must comput applic crime solv\n",
      "68 report actual studi even concern safeti manufactur employe instal worker fine diamet fiber use insul\n",
      "57 discuss mci bell breakup\n",
      "97 must instanc fiber optic technolog actual use\n",
      "98 must produc fiber optic equip\n",
      "60 either one use salari incent pay contrast sole basi senior longev job\n",
      "80 1988 presidenti\n",
      "63 machin translat\n",
      "91 acquisit armi weapon\n"
     ]
    }
   ],
   "source": [
    "query_file = '../IR_data/AP_DATA/query_desc.51-100.short.txt'\n",
    "query_map = {}\n",
    "with open(query_file, 'r') as f: \n",
    "    query_content = f.read().split('\\n')\n",
    "for line in query_content:\n",
    "    dot_index = line.index('.')\n",
    "    query_map[line[:dot_index]] = process_query(line[dot_index+1:].strip())\n",
    "for k,v in query_map.items():\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680ae37b-5613-418a-a7c4-d7f411ac1ad7",
   "metadata": {},
   "source": [
    "### ES Builtin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "311c5eb7-35a5-4ac3-8822-95a38f06b4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "ic = IndicesClient(es)\n",
    "print(es.ping())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a31b3e4a-d3c1-474d-a76c-fc7a45128ff1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'query_map' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/scores/es_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mq_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     19\u001b[0m             pickle\u001b[38;5;241m.\u001b[39mdump(scores, file)\n\u001b[0;32m---> 21\u001b[0m \u001b[43msave_es_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 16\u001b[0m, in \u001b[0;36msave_es_scores\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_es_scores\u001b[39m():    \n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m q_id \u001b[38;5;129;01min\u001b[39;00m \u001b[43mquery_map\u001b[49m:\n\u001b[1;32m     17\u001b[0m         scores \u001b[38;5;241m=\u001b[39m get_score(query_map[q_id])\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/scores/es_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mq_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'query_map' is not defined"
     ]
    }
   ],
   "source": [
    "def get_score(query):\n",
    "    res = es.search(index=index_name, query={'match': {'content': query}}, size=2000, scroll='2m')\n",
    "    scroll_id = res['_scroll_id']\n",
    "    scores = {}\n",
    "    while True:\n",
    "        for hit in res['hits']['hits']:\n",
    "            doc_id = hit['_id']\n",
    "            score = hit['_score']\n",
    "            scores[doc_id] = score\n",
    "        if len(res['hits']['hits']) == 0:\n",
    "            break\n",
    "        res = es.scroll(scroll_id=scroll_id, scroll='2m')\n",
    "    return scores\n",
    "\n",
    "def save_es_scores():    \n",
    "    for q_id in query_map:\n",
    "        scores = get_score(query_map[q_id])\n",
    "        with open(f'data/scores/es_{q_id}.pickle', 'wb') as file:\n",
    "            pickle.dump(scores, file)\n",
    "        \n",
    "save_es_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0c872e-d035-43c6-b0df-3c20de8f6bf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f42078bf-1d08-4082-b9c4-9e854f9e8bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = list(text_map.keys())\n",
    "number_of_docs = len(ids)\n",
    "def get_term_doc_frequencies():\n",
    "    size = 3000\n",
    "    term_frequencies = {}\n",
    "    doc_frequencies = {}\n",
    "    term_ttf = {}\n",
    "    \n",
    "    for i in range((number_of_docs // size)+1):\n",
    "        body = {\n",
    "            \"ids\": ids[size*i:min(number_of_docs,size*(i+1))],\n",
    "            \"parameters\": {\n",
    "                \"fields\": [\"content\"],\n",
    "                \"offsets\": \"false\",\n",
    "                \"payloads\": \"false\",\n",
    "                \"positions\": \"false\",\n",
    "                \"term_statistics\": \"true\",\n",
    "                \"field_statistics\": \"false\"\n",
    "            }\n",
    "        }\n",
    "        term_vectors = es.mtermvectors(index=index_name, body=body)\n",
    "        for doc in term_vectors['docs']:\n",
    "            doc_id = doc['_id']\n",
    "            if 'term_vectors' in doc and 'content' in doc['term_vectors']:\n",
    "                terms = doc['term_vectors']['content']['terms']\n",
    "                term_frequencies[doc_id] = {}\n",
    "                doc_frequencies[doc_id] = {}\n",
    "                term_ttf[doc_id] = {}\n",
    "                for term, info in terms.items():\n",
    "                    term_frequencies[doc_id][term] = info['term_freq']\n",
    "                    doc_frequencies[doc_id][term] = info['doc_freq']\n",
    "                    term_ttf[doc_id][term] = info['ttf']\n",
    "    return term_frequencies, doc_frequencies, term_ttf\n",
    "\n",
    "term_frequencies, doc_frequencies, term_ttf = get_term_doc_frequencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c5eb7a92-fdc2-4c29-94cf-51646e5f9d83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_len_d = 0\n",
    "for k,v in term_frequencies.items():\n",
    "    avg_len_d+=len(v)\n",
    "avg_len_d = avg_len_d//len(term_frequencies) #170.072\n",
    "avg_len_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "43bb3571-7cbb-4a29-ae89-3986c400490d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x4/d8y9fk1d22v8jm_mptcbdvv40000gn/T/ipykernel_8246/717131888.py:1: DeprecationWarning: The 'body' parameter is deprecated for the 'search' API and will be removed in a future version. Instead use API parameters directly. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  num_unique_words = es.search(index=index_name, body={ \"aggs\": { \"unique_terms\": { \"cardinality\": { \"field\": \"content\", \"precision_threshold\": 40000}}} })['aggregations']['unique_terms']['value']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "182041"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_unique_words = es.search(index=index_name, body={ \"aggs\": { \"unique_terms\": { \"cardinality\": { \"field\": \"content\", \"precision_threshold\": 40000}}} })['aggregations']['unique_terms']['value']\n",
    "num_unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "473d2682-6aa2-4586-acfa-3bd0b26f7602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_okapi_TF(tf, doc_length, avg_len_d):\n",
    "    return tf / (tf + 0.5 + 1.5 * (doc_length / avg_len_d))\n",
    "\n",
    "def calculate_tfidf(tf, df, doc_length, num_doc, avg_len_d):\n",
    "    term_f = (tf / (tf + 0.5 + 1.5 * (doc_length / avg_len_d)))\n",
    "    idf = math.log(num_doc / df) if df else 0\n",
    "    return  term_f * idf\n",
    "\n",
    "def compute_okapi_bm25(tf, df, doc_length, avg_len_d, num_doc):\n",
    "    k1, k2, b = 1.2, 1,0.75\n",
    "    return math.log((num_doc + 0.5) / (df + 0.5)) * ((tf + k1 * tf) / (tf + k1 * ((1 - b) + b * (doc_length / avg_len_d)))) * ((tf + k2 * tf)/(tf + k2))\n",
    "\n",
    "def compute_unigram_lml(tf, doc_length, num_unique_words):\n",
    "    if tf!=0:\n",
    "        return math.log((tf + 1) / (doc_length + num_unique_words))\n",
    "    else: \n",
    "        return -1000\n",
    "    \n",
    "def compute_unigram_lmjm(tf, ttf, doc_length, num_unique_words, lambda_const):\n",
    "    if tf!=0:\n",
    "        return math.log(lambda_const * (tf / doc_length) + (1 - lambda_const) * (ttf / num_unique_words))\n",
    "    else: \n",
    "        return -1000\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9b499151-7d88-4907-b2d4-25d350e3d3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(query_map, term_frequencies, doc_frequencies, term_ttf, avg_len_d, model):\n",
    "    model_scores = defaultdict(dict)\n",
    "    for doc_id, doc in term_frequencies.items():\n",
    "        len_d = len(doc)\n",
    "        num_doc = len(term_frequencies.keys())\n",
    "        for query_id, query in query_map.items():\n",
    "            score = 0\n",
    "            for term in query.split():\n",
    "                tf, df, ttf = 0, 0, 0\n",
    "                if term in doc: \n",
    "                    tf = doc[term]\n",
    "                    df = doc_frequencies[doc_id][term]\n",
    "                    ttf = term_ttf[doc_id][term]\n",
    "                if model=='okapi_tf':\n",
    "                    score += calculate_okapi_TF(tf, len_d, avg_len_d)\n",
    "                elif model=='tfidf':\n",
    "                    score += calculate_tfidf(tf, df, len_d, num_doc, avg_len_d)\n",
    "                elif model=='okapi_bm25':\n",
    "                    score += compute_okapi_bm25(tf, df, len_d, avg_len_d, num_doc)\n",
    "                elif model=='unigram_lml':\n",
    "                    score += compute_unigram_lml(tf, len_d, num_unique_words)\n",
    "                elif model=='unigram_lmjm':\n",
    "                    score += compute_unigram_lmjm(tf, ttf, len_d, num_unique_words, 0.5)\n",
    "            if score!=0:\n",
    "                if doc_id not in model_scores[query_id]:\n",
    "                    model_scores[query_id][doc_id] = score\n",
    "                else:\n",
    "                    model_scores[query_id][doc_id] += score\n",
    "    return model_scores\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b097e827",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_scores(model_scores, filename):\n",
    "    for q_id, scores in model_scores.items():\n",
    "        with open(f'data/scores/{filename}/{filename}_{q_id}.pickle', 'wb') as file:\n",
    "            pickle.dump(scores, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f17cc568-15d1-4c19-8d8e-056971be5525",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'okapi_tf'\n",
    "okapi_tf_model_scores = get_scores(query_map, term_frequencies, doc_frequencies, term_ttf, avg_len_d, model='okapi_tf')\n",
    "save_scores(okapi_tf_model_scores, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2d5d7a3d-15c7-4484-8f17-75dbe1af6e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'tfidf'\n",
    "tfidf_ranked_model_scores = get_scores(query_map, term_frequencies, doc_frequencies, term_ttf, avg_len_d, model='tfidf')\n",
    "save_scores(tfidf_ranked_model_scores, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "378ab3c0-50df-42a2-8478-b5cfc615759d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'okapi_bm25'\n",
    "okapi_bm25_model_scores = get_scores(query_map, term_frequencies, doc_frequencies, term_ttf, avg_len_d, model='okapi_bm25')\n",
    "save_scores(okapi_bm25_model_scores, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "43c42ed4-7ed4-4b18-b540-51fdca07455e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'unigram_lml'\n",
    "unigram_lml_model_scores = get_scores(query_map, term_frequencies, doc_frequencies, term_ttf, avg_len_d, model='unigram_lml')\n",
    "save_scores(unigram_lml_model_scores, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f7b13cf7-2cdc-4684-9239-cd259713ec56",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'unigram_lmjm'\n",
    "unigram_lmjm_model_scores = get_scores(query_map, term_frequencies, doc_frequencies, term_ttf, avg_len_d, model='unigram_lmjm')\n",
    "save_scores(unigram_lmjm_model_scores, filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

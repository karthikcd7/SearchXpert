{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60664c42-a7e6-4a3d-b485-120924c13054",
   "metadata": {},
   "source": [
    "# Assignment 1 \n",
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "efb494f9-7e59-4ac2-b394-42bd81a70b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from elasticsearch7 import Elasticsearch\n",
    "from elasticsearch7.client import IndicesClient\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ae8e38-843d-4f60-a661-99f93af0c336",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee34caa7-56a4-44c1-8ee1-c516fa0055f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "stoplist = '../IR_data/AP_DATA/stoplist.txt'\n",
    "with open(stoplist, 'r') as f:\n",
    "    stop_words = set(f.read().split())\n",
    "def stem_text_and_remove_stopwords(tokens):\n",
    "    processed_tokens = []\n",
    "    for word in tokens: \n",
    "        w = word.strip()\n",
    "        w = ps.stem(w)\n",
    "        if w.lower() not in stop_words:\n",
    "            processed_tokens.append(w)\n",
    "    return ' '.join(processed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30fbf0f1-bdec-4261-a06c-13811143acad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    processed_text = stem_text_and_remove_stopwords(tokens)\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "992812ac-b578-4334-95d1-6dd6bfc99bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents:  84678\n"
     ]
    }
   ],
   "source": [
    "text_map = defaultdict(str)\n",
    "folder = '../IR_data/AP_DATA/ap89_collection'\n",
    "count = 0\n",
    "for filename in os.listdir(folder):\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    with open(file_path, 'rb') as f:\n",
    "        content = f.read().decode('iso-8859-1')\n",
    "    doc_regex = r'<DOC>(.*?)</DOC>'\n",
    "    for doc in re.findall(doc_regex, content, re.S):\n",
    "        docno = re.search(r'<DOCNO>(.*?)</DOCNO>', doc).group(1).strip()      \n",
    "        for each in re.findall(r'<TEXT>(.*?)</TEXT>', doc, re.S):    \n",
    "            text_map[docno]+= ' ' + process_text(each)\n",
    "                \n",
    "print(\"Number of documents: \", len(text_map))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959a93ae-4908-44d5-b6ac-b1413783d53d",
   "metadata": {},
   "source": [
    "### ElasticSearch - Indexing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "590560b5-b2e7-411c-8963-fba21c3fadb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "es = Elasticsearch('http://localhost:9200/', timeout=60)\n",
    "print(es.ping())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f8336f3-984c-4377-9e4b-5b2f5e242e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = 'ap89_collection'\n",
    "\n",
    "configurations = {\n",
    "    \"settings\" : {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 1,\n",
    "        \"analysis\": {\n",
    "            \"filter\": {\n",
    "                \"english_stop\": {\n",
    "                    \"type\": \"stop\",\n",
    "                    \"stopwords_path\": \"my_stoplist.txt\"\n",
    "                }\n",
    "            },\n",
    "            \"analyzer\": {\n",
    "                \"stopped\": {\n",
    "                    \"type\": \"custom\",\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\",\n",
    "                        \"english_stop\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "      }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"content\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fielddata\": True,\n",
    "                \"analyzer\": \"stopped\",\n",
    "                \"index_options\": \"positions\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b2a13cd3-5fc2-4181-bd0f-0247b1f583c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x4/d8y9fk1d22v8jm_mptcbdvv40000gn/T/ipykernel_48151/3868789886.py:1: DeprecationWarning: The 'body' parameter is deprecated for the 'create' API and will be removed in a future version. Instead use API parameters directly. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  es.indices.create(index=index_name, body=configurations)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'ap89_collection'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.create(index=index_name, body=configurations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "92e9a056-04c9-46ce-9eb3-f1e5083e2a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_data(_id, text):\n",
    "    es.index(index=index_name, document={'content':text},id=_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "59f912cf-908d-441c-84dd-064d349202cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully index data!\n"
     ]
    }
   ],
   "source": [
    "for key in text_map: \n",
    "    add_data(key, text_map[key])\n",
    "print(\"Successfully index data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "16803850-b73f-44f6-b559-d92d9d7c641b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_shards': {'total': 2, 'successful': 1, 'failed': 0}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.refresh(index=index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bca3c9-a34a-48b5-8bc7-388fa11d70f9",
   "metadata": {},
   "source": [
    "## Task 3 - Retreival Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7f38d767-4e8f-466c-99b1-95b59eb8db34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess queries\n",
    "query_stop_words = stop_words.union(('document', 'noncommunist', 'locat', 'least', 'countri', 'second', 'unsubstanti', 'worldwid', 'exist', \n",
    "                               'product', 'preliminari', 'perpetr', 'aid', 'success', 'predict', 'describ', 'identifi', 'make', 'undesir',\n",
    "                               'level', 'determin', 'perform', 'platform', 'someth', 'side', 'effort', 'standard', 'motiv',\n",
    "                               'controversi', 'measur', 'tent', 'sign', 'individu', 'develop', 'nation', 'pend',\n",
    "                               'includ', 'result', 'anticip', 'support', 'ani', 'ha', 'directli', 'border' ,'area', 'base',\n",
    "                              'affair', 'ongo', 'method', 'sinc', 'system', 'candid', 'specifi', 'advanc', 'polit', 'attempt', 'asset'\n",
    "                              , 'organ','u s'))\n",
    "def query_stem_text_and_remove_stopwords(tokens):\n",
    "    processed_tokens = []\n",
    "    for word in tokens: \n",
    "        w = ps.stem(word)\n",
    "        w = w.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation))).strip()\n",
    "        if w!='' and w.lower() not in query_stop_words and w not in processed_tokens:\n",
    "            processed_tokens.append(w)\n",
    "    return ' '.join(processed_tokens)\n",
    "    \n",
    "def process_query(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    processed_tokens = query_stem_text_and_remove_stopwords(tokens)\n",
    "    return processed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "78007b20-1a51-4fc7-9ff1-2129ca036542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85 alleg corrupt public offici government jurisdict\n",
      "59 weather caus fatal\n",
      "56 prime lend rate\n",
      "71 incurs militari forc guerrilla\n",
      "64 hostage tak\n",
      "62 militari coup d etat\n",
      "93 rifl associ nra\n",
      "99 iran contra\n",
      "58 rail strike\n",
      "77 poach wildlif\n",
      "54 contract agreement reserv launch commerci satellit\n",
      "87 current crimin action offic fail financi institut\n",
      "94 crime comput\n",
      "100 non communist industri regul transfer high tech good dual us technolog\n",
      "89 invest opec downstream oper\n",
      "61 israel iran contra\n",
      "95 comput applic crime solv\n",
      "68 studi concern safeti manufactur employe instal worker fine diamet fiber insul\n",
      "57 mci bell\n",
      "97 instanc fiber optic technolog\n",
      "98 produc fiber optic equip\n",
      "60 salari incent pay contrast sole basi senior longev\n",
      "80 1988 presidenti\n",
      "63 machin translat\n",
      "91 acquisit armi weapon\n"
     ]
    }
   ],
   "source": [
    "query_file = '../IR_data/AP_DATA/query_desc.51-100.short.txt'\n",
    "query_map = {}\n",
    "with open(query_file, 'r') as f: \n",
    "    query_content = f.read().split('\\n')\n",
    "for line in query_content:\n",
    "    dot_index = line.index('.')\n",
    "    query_map[line[:dot_index]] = process_query(line[dot_index+1:].strip())\n",
    "for k,v in query_map.items():\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680ae37b-5613-418a-a7c4-d7f411ac1ad7",
   "metadata": {},
   "source": [
    "### ES Builtin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "311c5eb7-35a5-4ac3-8822-95a38f06b4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "ic = IndicesClient(es)\n",
    "print(es.ping())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a31b3e4a-d3c1-474d-a76c-fc7a45128ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ES_Search(query):\n",
    "    res_es_search = es.search(index=index_name, query={'match': {'content': query}}, size=1000)\n",
    "    return res_es_search\n",
    "\n",
    "def process_res(res, query_num):    \n",
    "    output = ''\n",
    "    for rank, data in enumerate(res['hits']['hits']):\n",
    "        output += query_num + ' Q0 ' + str(data['_id']) + ' ' + str(rank+1) + ' ' + str(data['_score']) + ' Exp\\n'\n",
    "    return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3a0c872e-d035-43c6-b0df-3c20de8f6bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = '../Deliverables/results/'\n",
    "\n",
    "def output_txt(filename, string):\n",
    "    with open(output_path+filename+'.txt', 'w') as f: \n",
    "        f.write(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b3364359-eb52-4872-8914-babd91542bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed query number 85\n",
      "Completed query number 59\n",
      "Completed query number 56\n",
      "Completed query number 71\n",
      "Completed query number 64\n",
      "Completed query number 62\n",
      "Completed query number 93\n",
      "Completed query number 99\n",
      "Completed query number 58\n",
      "Completed query number 77\n",
      "Completed query number 54\n",
      "Completed query number 87\n",
      "Completed query number 94\n",
      "Completed query number 100\n",
      "Completed query number 89\n",
      "Completed query number 61\n",
      "Completed query number 95\n",
      "Completed query number 68\n",
      "Completed query number 57\n",
      "Completed query number 97\n",
      "Completed query number 98\n",
      "Completed query number 60\n",
      "Completed query number 80\n",
      "Completed query number 63\n",
      "Completed query number 91\n",
      "Completed total run in time 1.67 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "filename = 'es_built_in'\n",
    "final_output = ''\n",
    "for query_num, query in query_map.items():\n",
    "    res = ES_Search(query)\n",
    "    print(\"Completed query number\", query_num)\n",
    "    output = process_res(res, query_num)\n",
    "    final_output+=output\n",
    "output_txt(filename, final_output)\n",
    "end = time.time()\n",
    "print(\"Completed total run in time\", round((end-start), 2), \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ddeb5fb1-876e-4d37-8655-bf700e1fda93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_txt(scores):\n",
    "    output=''\n",
    "    for query_id, doc_ids in scores.items():\n",
    "        for rank, (doc_id, score) in enumerate(doc_ids):\n",
    "            output += query_id + ' Q0 ' + str(doc_id) + ' ' + str(rank+1) + ' ' + str(score) + ' Exp\\n'\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc860319-4b85-4a39-927a-0f89ec316d4b",
   "metadata": {},
   "source": [
    "#### Getting tf, df, ttf from ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f42078bf-1d08-4082-b9c4-9e854f9e8bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = list(text_map.keys())\n",
    "number_of_docs = len(ids)\n",
    "def get_term_doc_frequencies():\n",
    "    size = 3000\n",
    "    term_frequencies = {}\n",
    "    doc_frequencies = {}\n",
    "    term_ttf = {}\n",
    "    \n",
    "    for i in range((number_of_docs // size)+1):\n",
    "        body = {\n",
    "            \"ids\": ids[size*i:min(number_of_docs,size*(i+1))],\n",
    "            \"parameters\": {\n",
    "                \"fields\": [\"content\"],\n",
    "                \"offsets\": \"false\",\n",
    "                \"payloads\": \"false\",\n",
    "                \"positions\": \"false\",\n",
    "                \"term_statistics\": \"true\",\n",
    "                \"field_statistics\": \"false\"\n",
    "            }    \n",
    "        }\n",
    "        term_vectors = es.mtermvectors(index=index_name, body=body)\n",
    "        for doc in term_vectors['docs']:\n",
    "            doc_id = doc['_id']\n",
    "            if 'term_vectors' in doc and 'content' in doc['term_vectors']:\n",
    "                terms = doc['term_vectors']['content']['terms']\n",
    "                term_frequencies[doc_id] = {}\n",
    "                doc_frequencies[doc_id] = {}\n",
    "                term_ttf[doc_id] = {}\n",
    "                for term, info in terms.items():\n",
    "                    term_frequencies[doc_id][term] = info['term_freq'] \n",
    "                    doc_frequencies[doc_id][term] = info['doc_freq']\n",
    "                    term_ttf[doc_id][term] = info['ttf']\n",
    "    return term_frequencies, doc_frequencies, term_ttf\n",
    "\n",
    "term_frequencies, doc_frequencies, term_ttf = get_term_doc_frequencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c5eb7a92-fdc2-4c29-94cf-51646e5f9d83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate average length of document\n",
    "avg_len_d = 0\n",
    "for k,v in term_frequencies.items():\n",
    "    avg_len_d+=len(v)\n",
    "avg_len_d = avg_len_d//len(term_frequencies) #170.072\n",
    "avg_len_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "43bb3571-7cbb-4a29-ae89-3986c400490d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x4/d8y9fk1d22v8jm_mptcbdvv40000gn/T/ipykernel_48151/3247321373.py:2: DeprecationWarning: The 'body' parameter is deprecated for the 'search' API and will be removed in a future version. Instead use API parameters directly. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  num_unique_words = es.search(index=index_name, body={ \"aggs\": { \"unique_terms\": { \"cardinality\": { \"field\": \"content\", \"precision_threshold\": 40000}}} })['aggregations']['unique_terms']['value']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "182041"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting Vocabulary size from ES\n",
    "num_unique_words = es.search(index=index_name, body={ \"aggs\": { \"unique_terms\": { \"cardinality\": { \"field\": \"content\", \"precision_threshold\": 40000}}} })['aggregations']['unique_terms']['value']\n",
    "num_unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "473d2682-6aa2-4586-acfa-3bd0b26f7602",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Okapi TF\n",
    "def calculate_okapi_TF(tf, doc_length, avg_len_d):\n",
    "    return tf / (tf + 0.5 + 1.5 * (doc_length / avg_len_d))\n",
    "\n",
    "#TFIDF\n",
    "def calculate_tfidf(tf, df, doc_length, num_doc, avg_len_d):\n",
    "    term_f = (tf / (tf + 0.5 + 1.5 * (doc_length / avg_len_d)))\n",
    "    idf = math.log(num_doc / df) if df else 0\n",
    "    return  term_f * idf\n",
    "\n",
    "#Okapi BM25\n",
    "def compute_okapi_bm25(tf, df, doc_length, avg_len_d, num_doc):\n",
    "    k1, k2, b = 1.2, 1,0.75\n",
    "    return math.log((num_doc + 0.5) / (df + 0.5)) * ((tf + k1 * tf) / (tf + k1 * ((1 - b) + b * (doc_length / avg_len_d)))) * ((tf + k2 * tf)/(tf + k2))\n",
    "\n",
    "#Unigram LM with Laplace smoothing\n",
    "def compute_unigram_lml(tf, doc_length, num_unique_words):\n",
    "    if tf!=0:\n",
    "        return math.log((tf + 1) / (doc_length + num_unique_words))\n",
    "    else: \n",
    "        return -1000\n",
    "\n",
    "# Unigram LM with Jelinek-Mercer smoothing\n",
    "def compute_unigram_lmjm(tf, ttf, doc_length, num_unique_words, lambda_const):\n",
    "    if tf!=0:\n",
    "        return math.log(lambda_const * (tf / doc_length) + (1 - lambda_const) * (ttf / num_unique_words))\n",
    "    else: \n",
    "        return -1000\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9b499151-7d88-4907-b2d4-25d350e3d3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rank documents and get top 1000 for each query\n",
    "def rank_documents(query_map, term_frequencies, doc_frequencies, term_ttf, avg_len_d, model):\n",
    "    model_scores = defaultdict(dict)\n",
    "    for doc_id, doc in term_frequencies.items():\n",
    "        len_d = len(doc)\n",
    "        num_doc = len(term_frequencies.keys())\n",
    "        for query_id, query in query_map.items():\n",
    "            score = 0\n",
    "            for term in query.split():\n",
    "                tf, df, ttf = 0, 0, 0\n",
    "                if term in doc: \n",
    "                    tf = doc[term]\n",
    "                    df = doc_frequencies[doc_id][term]\n",
    "                    ttf = term_ttf[doc_id][term]\n",
    "                if model=='okapi_tf':\n",
    "                    score += calculate_okapi_TF(tf, len_d, avg_len_d)\n",
    "                elif model=='tfidf':\n",
    "                    score += calculate_tfidf(tf, df, len_d, num_doc, avg_len_d)\n",
    "                elif model=='okapi_bm25':\n",
    "                    score += compute_okapi_bm25(tf, df, len_d, avg_len_d, num_doc)\n",
    "                elif model=='unigram_lml':\n",
    "                    score += compute_unigram_lml(tf, len_d, num_unique_words)\n",
    "                elif model=='unigram_lmjm':\n",
    "                    score += compute_unigram_lmjm(tf, ttf, len_d, num_unique_words, 0.5)\n",
    "            if score!=0:\n",
    "                if doc_id not in model_scores[query_id]:\n",
    "                    model_scores[query_id][doc_id] = score\n",
    "                else:\n",
    "                    model_scores[query_id][doc_id] += score\n",
    "    ranked_documents = {}\n",
    "    for query_id, doc_scores in model_scores.items():\n",
    "        ranked_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)[:1000]\n",
    "        ranked_documents[query_id] = ranked_docs\n",
    "    ranked_documents\n",
    "    return ranked_documents\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a59f8e5-6c30-458b-8de5-a54848a05cdf",
   "metadata": {},
   "source": [
    "## Save to files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a466be1-7476-46c6-b5dd-35675c110138",
   "metadata": {},
   "source": [
    "### Okapi TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f17cc568-15d1-4c19-8d8e-056971be5525",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'okapi_tf'\n",
    "okapi_tf_ranked_documents = rank_documents(query_map, term_frequencies, doc_frequencies, term_ttf, avg_len_d, model='okapi_tf')\n",
    "output = process_txt(okapi_tf_ranked_documents)\n",
    "output_txt(filename, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e803abc5-d633-46dc-9002-f50946d66310",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2d5d7a3d-15c7-4484-8f17-75dbe1af6e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'tfidf'\n",
    "tfidf_ranked_documents = rank_documents(query_map, term_frequencies, doc_frequencies, term_ttf, avg_len_d, model='tfidf')\n",
    "output = process_txt(tfidf_ranked_documents)\n",
    "output_txt(filename, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc10bcfd-a1c7-4da0-b062-49f3aadf4495",
   "metadata": {},
   "source": [
    "### Okapi BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "378ab3c0-50df-42a2-8478-b5cfc615759d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'okapi_bm25'\n",
    "okapi_bm25_ranked_documents = rank_documents(query_map, term_frequencies, doc_frequencies, term_ttf, avg_len_d, model='okapi_bm25')\n",
    "output = process_txt(okapi_bm25_ranked_documents)\n",
    "output_txt(filename, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59b9e24-f92f-4da5-8dde-7a4ba2d534ba",
   "metadata": {},
   "source": [
    "### Unigram LM with Laplace smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "43c42ed4-7ed4-4b18-b540-51fdca07455e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'unigram_lml'\n",
    "unigram_lml_ranked_documents = rank_documents(query_map, term_frequencies, doc_frequencies, term_ttf, avg_len_d, model='unigram_lml')\n",
    "output = process_txt(unigram_lml_ranked_documents)\n",
    "output_txt(filename, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd932aaf-d06c-46ca-8ebe-7258315acff8",
   "metadata": {},
   "source": [
    "### Unigram LM with Jelinek-Mercer smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f7b13cf7-2cdc-4684-9239-cd259713ec56",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'unigram_lmjm'\n",
    "unigram_lmjm_ranked_documents = rank_documents(query_map, term_frequencies, doc_frequencies, term_ttf, avg_len_d, model='unigram_lmjm')\n",
    "output = process_txt(unigram_lmjm_ranked_documents)\n",
    "output_txt(filename, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfab4b3-ad37-47cf-82bd-64a209ea8f6c",
   "metadata": {},
   "source": [
    "## Task 5 - Pseudo-relevance Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c2e40f67-c060-4950-b90b-c4c748daf97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevance_feedback_terms(k = 10):\n",
    "    relevance_feedback_terms = defaultdict(list)\n",
    "    for query_id, ranked_docs in tfidf_ranked_documents.items():\n",
    "        top = []\n",
    "        for doc_id, tfidf in ranked_docs[:3]:\n",
    "            top.extend(term_frequencies[doc_id].items())\n",
    "        relevance_feedback_terms[query_id] = [item[0] for item in sorted(top, key=lambda x: (x[1]), reverse=True)[:k] if len(item[0])>2]\n",
    "    return dict(relevance_feedback_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9c2a3cef-f9c0-46ed-b433-8673aa57f08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'85': 'alleg corrupt public offici government jurisdict',\n",
       " '59': 'weather caus fatal tornado concentr',\n",
       " '56': 'prime lend rate rate bank',\n",
       " '71': 'incurs militari forc guerrilla south',\n",
       " '64': 'hostage tak hostag lebanon unit',\n",
       " '62': 'militari coup d etat noriega',\n",
       " '93': 'rifl associ nra gun nra',\n",
       " '99': 'iran contra north iran mees',\n",
       " '58': 'rail strike rail union strike',\n",
       " '77': 'poach wildlif gator eleph allig',\n",
       " '54': 'contract agreement reserv launch commerci satellit',\n",
       " '87': 'current crimin action offic fail financi institut',\n",
       " '94': 'crime comput comput comput chip',\n",
       " '100': 'non communist industri regul transfer high tech good dual us technolog',\n",
       " '89': 'invest opec downstream oper million',\n",
       " '61': 'israel iran contra north north',\n",
       " '95': 'comput applic crime solv comput',\n",
       " '68': 'studi concern safeti manufactur employe instal worker fine diamet fiber insul',\n",
       " '57': 'mci bell mci amp amp',\n",
       " '97': 'instanc fiber optic technolog fiber',\n",
       " '98': 'produc fiber optic equip fiber',\n",
       " '60': 'salari incent pay contrast sole basi senior longev',\n",
       " '80': '1988 presidenti hart democrat busi',\n",
       " '63': 'machin translat japan fax helm',\n",
       " '91': 'acquisit armi weapon cost billion'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevance_feedback_terms = get_relevance_feedback_terms()\n",
    "query_map_with_relevance_feedback = query_map.copy()\n",
    "for query_id, query in query_map_with_relevance_feedback.items():\n",
    "    relevance = query.split()\n",
    "    i = 0\n",
    "    while len(relevance)<5:\n",
    "        relevance.append(relevance_feedback_terms[query_id][i])\n",
    "        i+=1\n",
    "    new_query = ' '.join(relevance)\n",
    "    query_map_with_relevance_feedback[query_id]=new_query\n",
    "query_map_with_relevance_feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74307ca9-918f-42af-be1b-90e8595f8b7f",
   "metadata": {},
   "source": [
    "### Okapi TF Pseudo-relevance Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5aa68a4a-67d7-4eaa-8c0c-cb1469b885bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'okapi_tf_rel_feedback'\n",
    "okapi_tf_rel_feedback_ranked_documents = rank_documents(query_map_with_relevance_feedback, term_frequencies, doc_frequencies, term_ttf, avg_len_d, model='okapi_tf')\n",
    "output = process_txt(okapi_tf_rel_feedback_ranked_documents)\n",
    "output_txt(filename, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb65ed35-0d02-44d3-836b-0b4a7057e7f5",
   "metadata": {},
   "source": [
    "###  ElasticSearch aggs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "62ce42ce-6abb-4c43-9ed1-d732c1eefba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rel_feedback_aggs(query):\n",
    "    significant_terms = es.search(index=index_name, \n",
    "                                  body = {\n",
    "                                      \"aggregations\" : {\n",
    "                                          \"significantTerms\" : {\n",
    "                                              \"significant_terms\" : {\n",
    "                                                  \"field\" : \"content\"\n",
    "                                              }\n",
    "                                          }\n",
    "                                      },\n",
    "                                       \"query\" : {\n",
    "                                          \"terms\" : {\n",
    "                                              \"content\" : [query.split()[-2]]\n",
    "                                          }\n",
    "                                      },\n",
    "                                      \"size\": 0\n",
    "                                  }\n",
    "                                 )\n",
    "    return significant_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "764cad6d-a573-4131-9ebb-b7588e40fd8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x4/d8y9fk1d22v8jm_mptcbdvv40000gn/T/ipykernel_48151/4122318170.py:2: DeprecationWarning: The 'body' parameter is deprecated for the 'search' API and will be removed in a future version. Instead use API parameters directly. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  significant_terms = es.search(index=index_name,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'85': 'alleg corrupt public offici government jurisdict',\n",
       " '59': 'weather caus fatal',\n",
       " '56': 'prime lend rate',\n",
       " '71': 'incurs militari forc guerrilla',\n",
       " '64': 'hostage hostage tak',\n",
       " '62': 'militari coup d etat',\n",
       " '93': 'rifl associ nra',\n",
       " '99': 'iran iran contra',\n",
       " '58': 'rail rail strike',\n",
       " '77': 'poach poach wildlif',\n",
       " '54': 'contract agreement reserv launch commerci satellit',\n",
       " '87': 'current crimin action offic fail financi institut',\n",
       " '94': 'crime crime comput',\n",
       " '100': 'non communist industri regul transfer high tech good dual us technolog',\n",
       " '89': 'invest opec downstream oper',\n",
       " '61': 'israel iran contra',\n",
       " '95': 'comput applic crime solv',\n",
       " '68': 'studi concern safeti manufactur employe instal worker fine diamet fiber insul',\n",
       " '57': 'mci mci bell',\n",
       " '97': 'instanc fiber optic technolog',\n",
       " '98': 'produc fiber optic equip',\n",
       " '60': 'salari incent pay contrast sole basi senior longev',\n",
       " '80': '1988 1988 presidenti',\n",
       " '63': 'machin machin translat',\n",
       " '91': 'acquisit armi weapon'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_map_with_relevance_feedback_aggs = query_map.copy()\n",
    "for query_id, query in query_map_with_relevance_feedback_aggs.items():\n",
    "    significant_terms_response = get_rel_feedback_aggs(query)\n",
    "    significant_terms=[]\n",
    "    for i in significant_terms_response['aggregations']['significantTerms']['buckets']: \n",
    "        if len(i['key'])>2:\n",
    "            significant_terms.append(i['key'])\n",
    "    relevance = query.split()\n",
    "    i = 0\n",
    "    while len(relevance)<3:\n",
    "        relevance.insert(0, significant_terms[i])\n",
    "        i+=1\n",
    "    new_query = ' '.join(relevance)\n",
    "    query_map_with_relevance_feedback_aggs[query_id]=new_query\n",
    "query_map_with_relevance_feedback_aggs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ba82ad-c186-4079-bb51-cd340b6a3bd8",
   "metadata": {},
   "source": [
    "### Okapi TF Pseudo-relevance Feedback using ElasticSearch aggs \"significant terms\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2ddf8fcf-1284-4a60-8d4b-e2af48d33c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'okapi_tf_rel_feedback_aggs'\n",
    "okapi_tf_rel_feedback_aggs_ranked_documents = rank_documents(query_map_with_relevance_feedback_aggs, term_frequencies, doc_frequencies, term_ttf, avg_len_d, model='okapi_tf')\n",
    "output = process_txt(okapi_tf_rel_feedback_aggs_ranked_documents)\n",
    "output_txt(filename, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596bdf20-2b13-4ba4-90af-72df950244ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

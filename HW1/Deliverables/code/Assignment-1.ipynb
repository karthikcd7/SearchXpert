{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60664c42-a7e6-4a3d-b485-120924c13054",
   "metadata": {},
   "source": [
    "# Assignment 1 \n",
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efb494f9-7e59-4ac2-b394-42bd81a70b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from elasticsearch7 import Elasticsearch\n",
    "from elasticsearch7.client import IndicesClient\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee34caa7-56a4-44c1-8ee1-c516fa0055f5",
   "metadata": {},
   "source": [
    "ps = PorterStemmer()\n",
    "stoplist = '../../IR_data/AP_DATA/stoplist.txt'\n",
    "with open(stoplist, 'r') as f:\n",
    "    stop_words = set(f.read().split())\n",
    "def stem_text_and_remove_stopwords(tokens):\n",
    "    processed_tokens = []\n",
    "    for word in tokens: \n",
    "        w = word.strip()\n",
    "        w = ps.stem(w)\n",
    "        if w.lower() not in stop_words:\n",
    "            processed_tokens.append(w)\n",
    "    return ' '.join(processed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30fbf0f1-bdec-4261-a06c-13811143acad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    processed_text = stem_text_and_remove_stopwords(tokens)\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "992812ac-b578-4334-95d1-6dd6bfc99bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84678\n"
     ]
    }
   ],
   "source": [
    "text_map = defaultdict(str)\n",
    "folder = '../../IR_data/AP_DATA/ap89_collection'\n",
    "count = 0\n",
    "for filename in os.listdir(folder):\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    with open(file_path, 'rb') as f:\n",
    "        content = f.read().decode('iso-8859-1')\n",
    "    doc_regex = r'<DOC>(.*?)</DOC>'\n",
    "    for doc in re.findall(doc_regex, content, re.S):\n",
    "        docno = re.search(r'<DOCNO>(.*?)</DOCNO>', doc).group(1).strip()      \n",
    "        for each in re.findall(r'<TEXT>(.*?)</TEXT>', doc, re.S):    \n",
    "            text_map[docno]+= ' ' + process_text(each)\n",
    "                \n",
    "print(len(text_map))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959a93ae-4908-44d5-b6ac-b1413783d53d",
   "metadata": {},
   "source": [
    "### ElasticSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "590560b5-b2e7-411c-8963-fba21c3fadb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "es = Elasticsearch('http://localhost:9200/', timeout=60)\n",
    "print(es.ping())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f8336f3-984c-4377-9e4b-5b2f5e242e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = 'ap89_collection0'\n",
    "\n",
    "configurations = {\n",
    "    \"settings\" : {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 1,\n",
    "        \"analysis\": {\n",
    "            \"filter\": {\n",
    "                \"english_stop\": {\n",
    "                    \"type\": \"stop\",\n",
    "                    \"stopwords_path\": \"my_stoplist.txt\"\n",
    "                }\n",
    "            },\n",
    "            \"analyzer\": {\n",
    "                \"stopped\": {\n",
    "                    \"type\": \"custom\",\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\",\n",
    "                        \"english_stop\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "      }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"content\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fielddata\": True,\n",
    "                \"analyzer\": \"stopped\",\n",
    "                \"index_options\": \"positions\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2a13cd3-5fc2-4181-bd0f-0247b1f583c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x4/d8y9fk1d22v8jm_mptcbdvv40000gn/T/ipykernel_47970/3868789886.py:1: DeprecationWarning: The 'body' parameter is deprecated for the 'create' API and will be removed in a future version. Instead use API parameters directly. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  es.indices.create(index=index_name, body=configurations)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True,\n",
       " 'shards_acknowledged': True,\n",
       " 'index': 'ap89_collection0'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.create(index=index_name, body=configurations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92e9a056-04c9-46ce-9eb3-f1e5083e2a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_data(_id, text):\n",
    "    es.index(index=index_name, document={'content':text},id=_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59f912cf-908d-441c-84dd-064d349202cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for key in text_map: \n",
    "    add_data(key, text_map[key])\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16803850-b73f-44f6-b559-d92d9d7c641b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_shards': {'total': 2, 'successful': 1, 'failed': 0}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.refresh(index=index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bca3c9-a34a-48b5-8bc7-388fa11d70f9",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f38d767-4e8f-466c-99b1-95b59eb8db34",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_stop_words = stop_words.union(('document', 'noncommunist', 'locat', 'least', 'countri', 'second', 'unsubstanti', 'worldwid', 'exist', \n",
    "                               'product', 'preliminari', 'perpetr', 'aid', 'success', 'predict', 'describ', 'identifi', 'make', 'undesir',\n",
    "                               'level', 'determin', 'perform', 'platform', 'someth', 'side', 'effort', 'standard', 'motiv',\n",
    "                               'controversi', 'measur', 'tent', 'sign', 'individu', 'develop', 'nation', 'pend',\n",
    "                               'includ', 'result', 'anticip', 'support', 'ani', 'ha', 'directli', 'border' ,'area', 'base',\n",
    "                              'affair', 'ongo', 'method', 'sinc', 'system', 'candid', 'specifi', 'advanc', 'polit', 'attempt', 'asset'\n",
    "                              , 'organ','u s'))\n",
    "def query_stem_text_and_remove_stopwords(tokens):\n",
    "    processed_tokens = []\n",
    "    for word in tokens: \n",
    "        w = ps.stem(word)\n",
    "        w = w.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation))).strip()\n",
    "        if w!='' and w.lower() not in query_stop_words and w not in processed_tokens:\n",
    "            processed_tokens.append(w)\n",
    "    return ' '.join(processed_tokens)\n",
    "    \n",
    "def process_query(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    processed_tokens = query_stem_text_and_remove_stopwords(tokens)\n",
    "    return processed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78007b20-1a51-4fc7-9ff1-2129ca036542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85 alleg corrupt public offici government jurisdict\n",
      "59 weather caus fatal\n",
      "56 prime lend rate\n",
      "71 incurs militari forc guerrilla\n",
      "64 hostage tak\n",
      "62 militari coup d etat\n",
      "93 rifl associ nra\n",
      "99 iran contra\n",
      "58 rail strike\n",
      "77 poach wildlif\n",
      "54 contract agreement reserv launch commerci satellit\n",
      "87 current crimin action offic fail financi institut\n",
      "94 crime comput\n",
      "100 non communist industri regul transfer high tech good dual us technolog\n",
      "89 invest opec downstream oper\n",
      "61 israel iran contra\n",
      "95 comput applic crime solv\n",
      "68 studi concern safeti manufactur employe instal worker fine diamet fiber insul\n",
      "57 mci bell\n",
      "97 instanc fiber optic technolog\n",
      "98 produc fiber optic equip\n",
      "60 salari incent pay contrast sole basi senior longev\n",
      "80 1988 presidenti\n",
      "63 machin translat\n",
      "91 acquisit armi weapon\n"
     ]
    }
   ],
   "source": [
    "query_file = '../../IR_data/AP_DATA/query_desc.51-100.short.txt'\n",
    "query_map = {}\n",
    "with open(query_file, 'r') as f: \n",
    "    query_content = f.read().split('\\n')\n",
    "for line in query_content:\n",
    "    dot_index = line.index('.')\n",
    "    query_map[line[:dot_index]] = process_query(line[dot_index+1:].strip())\n",
    "for k,v in query_map.items():\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680ae37b-5613-418a-a7c4-d7f411ac1ad7",
   "metadata": {},
   "source": [
    "### ES Builtin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "311c5eb7-35a5-4ac3-8822-95a38f06b4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "ic = IndicesClient(es)\n",
    "print(es.ping())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a31b3e4a-d3c1-474d-a76c-fc7a45128ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ES_Search(query):\n",
    "    res_es_search = es.search(index=index_name, query={'match': {'content': query}}, size=1000)\n",
    "    return res_es_search\n",
    "\n",
    "def process_res(res, query_num):    \n",
    "    output = ''\n",
    "    for rank, data in enumerate(res['hits']['hits']):\n",
    "        output += query_num + ' Q0 ' + str(data['_id']) + ' ' + str(rank+1) + ' ' + str(data['_score']) + ' Exp\\n'\n",
    "    return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a0c872e-d035-43c6-b0df-3c20de8f6bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = '../Deliverables/'\n",
    "\n",
    "def output_txt(filename, string):\n",
    "    with open(output_path+filename+'.txt', 'w') as f: \n",
    "        f.write(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3364359-eb52-4872-8914-babd91542bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed query number 85\n",
      "Completed query number 59\n",
      "Completed query number 56\n",
      "Completed query number 71\n",
      "Completed query number 64\n",
      "Completed query number 62\n",
      "Completed query number 93\n",
      "Completed query number 99\n",
      "Completed query number 58\n",
      "Completed query number 77\n",
      "Completed query number 54\n",
      "Completed query number 87\n",
      "Completed query number 94\n",
      "Completed query number 100\n",
      "Completed query number 89\n",
      "Completed query number 61\n",
      "Completed query number 95\n",
      "Completed query number 68\n",
      "Completed query number 57\n",
      "Completed query number 97\n",
      "Completed query number 98\n",
      "Completed query number 60\n",
      "Completed query number 80\n",
      "Completed query number 63\n",
      "Completed query number 91\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../Deliverables/es_built_in.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m     output \u001b[38;5;241m=\u001b[39m process_res(res, query_num)\n\u001b[1;32m      8\u001b[0m     final_output\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39moutput\n\u001b[0;32m----> 9\u001b[0m \u001b[43moutput_txt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted total run in time\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mround\u001b[39m((end\u001b[38;5;241m-\u001b[39mstart), \u001b[38;5;241m2\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[19], line 4\u001b[0m, in \u001b[0;36moutput_txt\u001b[0;34m(filename, string)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moutput_txt\u001b[39m(filename, string):\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f: \n\u001b[1;32m      5\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(string)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Deliverables/es_built_in.txt'"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "filename = 'es_built_in'\n",
    "final_output = ''\n",
    "for query_num, query in query_map.items():\n",
    "    res = ES_Search(query)\n",
    "    print(\"Completed query number\", query_num)\n",
    "    output = process_res(res, query_num)\n",
    "    final_output+=output\n",
    "output_txt(filename, final_output)\n",
    "end = time.time()\n",
    "print(\"Completed total run in time\", round((end-start), 2), \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeb5fb1-876e-4d37-8655-bf700e1fda93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_txt(scores):\n",
    "    output=''\n",
    "    for query_id, doc_ids in scores.items():\n",
    "        for rank, (doc_id, score) in enumerate(doc_ids):\n",
    "            output += query_id + ' Q0 ' + str(doc_id) + ' ' + str(rank+1) + ' ' + str(score) + ' Exp\\n'\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42078bf-1d08-4082-b9c4-9e854f9e8bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = list(text_map.keys())\n",
    "number_of_docs = len(ids)\n",
    "def get_term_doc_frequencies():\n",
    "    size = 3000\n",
    "    term_frequencies = {}\n",
    "    doc_frequencies = {}\n",
    "    term_ttf = {}\n",
    "    \n",
    "    for i in range((number_of_docs // size)+1):\n",
    "        body = {\n",
    "            \"ids\": ids[size*i:min(number_of_docs,size*(i+1))],\n",
    "            \"parameters\": {\n",
    "                \"fields\": [\"content\"],\n",
    "                \"offsets\": \"false\",\n",
    "                \"payloads\": \"false\",\n",
    "                \"positions\": \"false\",\n",
    "                \"term_statistics\": \"true\",\n",
    "                \"field_statistics\": \"false\"\n",
    "            }    \n",
    "        }\n",
    "        term_vectors = es.mtermvectors(index=index_name, body=body)\n",
    "        print(term_vectors)\n",
    "        for doc in term_vectors['docs']:\n",
    "            doc_id = doc['_id']\n",
    "            if 'term_vectors' in doc and 'content' in doc['term_vectors']:\n",
    "                terms = doc['term_vectors']['content']['terms']\n",
    "                term_frequencies[doc_id] = {}\n",
    "                doc_frequencies[doc_id] = {}\n",
    "                term_ttf[doc_id] = {}\n",
    "                for term, info in terms.items():\n",
    "                    term_frequencies[doc_id][term] = info['term_freq'] \n",
    "                    doc_frequencies[doc_id][term] = info['doc_freq']\n",
    "                    term_ttf[doc_id][term] = info['ttf']\n",
    "    return term_frequencies, doc_frequencies, term_ttf\n",
    "\n",
    "term_frequencies, doc_frequencies, term_ttf = get_term_doc_frequencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5eb7a92-fdc2-4c29-94cf-51646e5f9d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_len_d = 0\n",
    "for k,v in term_frequencies.items():\n",
    "    avg_len_d+=len(v)\n",
    "avg_len_d = avg_len_d//len(term_frequencies) #170.072\n",
    "avg_len_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bb3571-7cbb-4a29-ae89-3986c400490d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unique_words = es.search(index=index_name, body={ \"aggs\": { \"unique_terms\": { \"cardinality\": { \"field\": \"content\", \"precision_threshold\": 40000}}} })['aggregations']['unique_terms']['value']\n",
    "num_unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473d2682-6aa2-4586-acfa-3bd0b26f7602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_okapi_TF(tf, doc_length, avg_len_d):\n",
    "    return tf / (tf + 0.5 + 1.5 * (doc_length / avg_len_d))\n",
    "\n",
    "def calculate_tfidf(tf, df, doc_length, num_doc, avg_len_d):\n",
    "    term_f = (tf / (tf + 0.5 + 1.5 * (doc_length / avg_len_d)))\n",
    "    idf = math.log(num_doc / df) if df else 0\n",
    "    return  term_f * idf\n",
    "\n",
    "def compute_okapi_bm25(tf, df, doc_length, avg_len_d, num_doc):\n",
    "    k1, k2, b = 1.2, 1,0.75\n",
    "    return math.log((num_doc + 0.5) / (df + 0.5)) * ((tf + k1 * tf) / (tf + k1 * ((1 - b) + b * (doc_length / avg_len_d)))) * ((tf + k2 * tf)/(tf + k2))\n",
    "\n",
    "def compute_unigram_lml(tf, doc_length, num_unique_words):\n",
    "    if tf!=0:\n",
    "        return math.log((tf + 1) / (doc_length + num_unique_words))\n",
    "    else: \n",
    "        return -1000\n",
    "    \n",
    "def compute_unigram_lmjm(tf, ttf, doc_length, num_unique_words, lambda_const):\n",
    "    if tf!=0:\n",
    "        return math.log(lambda_const * (tf / doc_length) + (1 - lambda_const) * (ttf / num_unique_words))\n",
    "    else: \n",
    "        return -1000\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9b499151-7d88-4907-b2d4-25d350e3d3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_documents(query_map, term_frequencies, doc_frequencies, term_ttf, avg_len_d, model):\n",
    "    model_scores = defaultdict(dict)\n",
    "    for doc_id, doc in term_frequencies.items():\n",
    "        len_d = len(doc)\n",
    "        num_doc = len(term_frequencies.keys())\n",
    "        for query_id, query in query_map.items():\n",
    "            score = 0\n",
    "            for term in query.split():\n",
    "                tf, df, ttf = 0, 0, 0\n",
    "                if term in doc: \n",
    "                    tf = doc[term]\n",
    "                    df = doc_frequencies[doc_id][term]\n",
    "                    ttf = term_ttf[doc_id][term]\n",
    "                if model=='okapi_tf':\n",
    "                    score += calculate_okapi_TF(tf, len_d, avg_len_d)\n",
    "                elif model=='tfidf':\n",
    "                    score += calculate_tfidf(tf, df, len_d, num_doc, avg_len_d)\n",
    "                elif model=='okapi_bm25':\n",
    "                    score += compute_okapi_bm25(tf, df, len_d, avg_len_d, num_doc)\n",
    "                elif model=='unigram_lml':\n",
    "                    score += compute_unigram_lml(tf, len_d, num_unique_words)\n",
    "                elif model=='unigram_lmjm':\n",
    "                    score += compute_unigram_lmjm(tf, ttf, len_d, num_unique_words, 0.5)\n",
    "            if score!=0:\n",
    "                if doc_id not in model_scores[query_id]:\n",
    "                    model_scores[query_id][doc_id] = score\n",
    "                else:\n",
    "                    model_scores[query_id][doc_id] += score\n",
    "    ranked_documents = {}\n",
    "    for query_id, doc_scores in model_scores.items():\n",
    "        ranked_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)[:1000]\n",
    "        ranked_documents[query_id] = ranked_docs\n",
    "    ranked_documents\n",
    "    return ranked_documents\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f17cc568-15d1-4c19-8d8e-056971be5525",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'okapi_tf'\n",
    "okapi_tf_ranked_documents = rank_documents(query_map, term_frequencies, doc_frequencies, term_ttf, avg_len_d, model='okapi_tf')\n",
    "output = process_txt(okapi_tf_ranked_documents)\n",
    "output_txt(filename, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2d5d7a3d-15c7-4484-8f17-75dbe1af6e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'tfidf'\n",
    "tfidf_ranked_documents = rank_documents(query_map, term_frequencies, doc_frequencies, term_ttf, avg_len_d, model='tfidf')\n",
    "output = process_txt(tfidf_ranked_documents)\n",
    "output_txt(filename, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "378ab3c0-50df-42a2-8478-b5cfc615759d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'okapi_bm25'\n",
    "okapi_bm25_ranked_documents = rank_documents(query_map, term_frequencies, doc_frequencies, term_ttf, avg_len_d, model='okapi_bm25')\n",
    "output = process_txt(okapi_bm25_ranked_documents)\n",
    "output_txt(filename, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "43c42ed4-7ed4-4b18-b540-51fdca07455e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'unigram_lml'\n",
    "unigram_lml_ranked_documents = rank_documents(query_map, term_frequencies, doc_frequencies, term_ttf, avg_len_d, model='unigram_lml')\n",
    "output = process_txt(unigram_lml_ranked_documents)\n",
    "output_txt(filename, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f7b13cf7-2cdc-4684-9239-cd259713ec56",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'unigram_lmjm'\n",
    "unigram_lmjm_ranked_documents = rank_documents(query_map, term_frequencies, doc_frequencies, term_ttf, avg_len_d, model='unigram_lmjm')\n",
    "output = process_txt(unigram_lmjm_ranked_documents)\n",
    "output_txt(filename, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfab4b3-ad37-47cf-82bd-64a209ea8f6c",
   "metadata": {},
   "source": [
    "## Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c2e40f67-c060-4950-b90b-c4c748daf97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevance_feedback_terms(k = 10):\n",
    "    relevance_feedback_terms = defaultdict(list)\n",
    "    for query_id, ranked_docs in tfidf_ranked_documents.items():\n",
    "        top = []\n",
    "        for doc_id, tfidf in ranked_docs[:3]:\n",
    "            top.extend(term_frequencies[doc_id].items())\n",
    "        relevance_feedback_terms[query_id] = [item[0] for item in sorted(top, key=lambda x: (x[1]), reverse=True)[:k] if len(item[0])>2]\n",
    "    return dict(relevance_feedback_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "9c2a3cef-f9c0-46ed-b433-8673aa57f08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'85': 'alleg corrupt public offici government jurisdict',\n",
       " '59': 'weather caus fatal tornado concentr',\n",
       " '56': 'prime lend rate rate bank',\n",
       " '71': 'incurs militari forc guerrilla south',\n",
       " '64': 'hostage tak hostag lebanon unit',\n",
       " '62': 'militari coup d etat noriega',\n",
       " '93': 'rifl associ nra gun nra',\n",
       " '99': 'iran contra north iran mees',\n",
       " '58': 'rail strike rail union strike',\n",
       " '77': 'poach wildlif gator eleph allig',\n",
       " '54': 'contract agreement reserv launch commerci satellit',\n",
       " '87': 'current crimin action offic fail financi institut',\n",
       " '94': 'crime comput comput comput chip',\n",
       " '100': 'non communist industri regul transfer high tech good dual us technolog',\n",
       " '89': 'invest opec downstream oper million',\n",
       " '61': 'israel iran contra north north',\n",
       " '95': 'comput applic crime solv comput',\n",
       " '68': 'studi concern safeti manufactur employe instal worker fine diamet fiber insul',\n",
       " '57': 'mci bell mci amp amp',\n",
       " '97': 'instanc fiber optic technolog fiber',\n",
       " '98': 'produc fiber optic equip fiber',\n",
       " '60': 'salari incent pay contrast sole basi senior longev',\n",
       " '80': '1988 presidenti hart democrat busi',\n",
       " '63': 'machin translat japan fax helm',\n",
       " '91': 'acquisit armi weapon cost billion'}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevance_feedback_terms = get_relevance_feedback_terms()\n",
    "query_map_with_relevance_feedback = query_map.copy()\n",
    "for query_id, query in query_map_with_relevance_feedback.items():\n",
    "    relevance = query.split()\n",
    "    i = 0\n",
    "    while len(relevance)<5:\n",
    "        relevance.append(relevance_feedback_terms[query_id][i])\n",
    "        i+=1\n",
    "    new_query = ' '.join(relevance)\n",
    "    query_map_with_relevance_feedback[query_id]=new_query\n",
    "query_map_with_relevance_feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5708de-9947-4907-aadd-4e299c2f6554",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5aa68a4a-67d7-4eaa-8c0c-cb1469b885bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'okapi_tf_rel_feedback'\n",
    "okapi_tf_rel_feedback_ranked_documents = rank_documents(query_map_with_relevance_feedback, term_frequencies, doc_frequencies, term_ttf, avg_len_d, model='okapi_tf')\n",
    "output = process_txt(okapi_tf_rel_feedback_ranked_documents)\n",
    "output_txt(filename, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a74da6-2378-4893-82b1-185250053182",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb65ed35-0d02-44d3-836b-0b4a7057e7f5",
   "metadata": {},
   "source": [
    "###  ElasticSearch aggs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "62ce42ce-6abb-4c43-9ed1-d732c1eefba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rel_feedback_aggs(query):\n",
    "    significant_terms = es.search(index=index_name, \n",
    "                                  body = {\n",
    "                                      \"aggregations\" : {\n",
    "                                          \"significantTerms\" : {\n",
    "                                              \"significant_terms\" : {\n",
    "                                                  \"field\" : \"content\"\n",
    "                                              }\n",
    "                                          }\n",
    "                                      },\n",
    "                                       \"query\" : {\n",
    "                                          \"terms\" : {\n",
    "                                              \"content\" : [query.split()[-2]]\n",
    "                                          }\n",
    "                                      },\n",
    "                                      \"size\": 0\n",
    "                                  }\n",
    "                                 )\n",
    "    return significant_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "764cad6d-a573-4131-9ebb-b7588e40fd8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x4/d8y9fk1d22v8jm_mptcbdvv40000gn/T/ipykernel_36931/4122318170.py:2: DeprecationWarning: The 'body' parameter is deprecated for the 'search' API and will be removed in a future version. Instead use API parameters directly. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  significant_terms = es.search(index=index_name,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'85': 'alleg corrupt public offici government jurisdict',\n",
       " '59': 'weather caus fatal',\n",
       " '56': 'prime lend rate',\n",
       " '71': 'incurs militari forc guerrilla',\n",
       " '64': 'hostage hostage tak',\n",
       " '62': 'militari coup d etat',\n",
       " '93': 'rifl associ nra',\n",
       " '99': 'iran iran contra',\n",
       " '58': 'rail rail strike',\n",
       " '77': 'poach poach wildlif',\n",
       " '54': 'contract agreement reserv launch commerci satellit',\n",
       " '87': 'current crimin action offic fail financi institut',\n",
       " '94': 'crime crime comput',\n",
       " '100': 'non communist industri regul transfer high tech good dual us technolog',\n",
       " '89': 'invest opec downstream oper',\n",
       " '61': 'israel iran contra',\n",
       " '95': 'comput applic crime solv',\n",
       " '68': 'studi concern safeti manufactur employe instal worker fine diamet fiber insul',\n",
       " '57': 'mci mci bell',\n",
       " '97': 'instanc fiber optic technolog',\n",
       " '98': 'produc fiber optic equip',\n",
       " '60': 'salari incent pay contrast sole basi senior longev',\n",
       " '80': '1988 1988 presidenti',\n",
       " '63': 'machin machin translat',\n",
       " '91': 'acquisit armi weapon'}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_map_with_relevance_feedback_aggs = query_map.copy()\n",
    "for query_id, query in query_map_with_relevance_feedback_aggs.items():\n",
    "    significant_terms_response = get_rel_feedback_aggs(query)\n",
    "    significant_terms=[]\n",
    "    for i in significant_terms_response['aggregations']['significantTerms']['buckets']: \n",
    "        if len(i['key'])>2:\n",
    "            significant_terms.append(i['key'])\n",
    "    relevance = query.split()\n",
    "    i = 0\n",
    "    while len(relevance)<3:\n",
    "        relevance.insert(0, significant_terms[i])\n",
    "        i+=1\n",
    "    new_query = ' '.join(relevance)\n",
    "    query_map_with_relevance_feedback_aggs[query_id]=new_query\n",
    "query_map_with_relevance_feedback_aggs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "2ddf8fcf-1284-4a60-8d4b-e2af48d33c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'okapi_tf_rel_feedback_aggs'\n",
    "okapi_tf_rel_feedback_aggs_ranked_documents = rank_documents(query_map_with_relevance_feedback_aggs, term_frequencies, doc_frequencies, term_ttf, avg_len_d, model='okapi_tf')\n",
    "output = process_txt(okapi_tf_rel_feedback_aggs_ranked_documents)\n",
    "output_txt(filename, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596bdf20-2b13-4ba4-90af-72df950244ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff813c60-892e-4140-a7ca-4d762450ac9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ff0877-ebc3-4aaf-9395-2ab489a181c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

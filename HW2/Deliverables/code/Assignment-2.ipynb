{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60664c42-a7e6-4a3d-b485-120924c13054",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efb494f9-7e59-4ac2-b394-42bd81a70b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import string\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import math\n",
    "import gzip \n",
    "output_files_path = \"../output/\"\n",
    "ids_output_files_path = \"../output/ids/\"\n",
    "without_stemming_output_files_path = \"../output/without_stemming/\"\n",
    "with_stemming_output_fils_path = \"../output/with_stemming/\"\n",
    "merged_files_path = output_files_path+'merged_files/'\n",
    "output_query_res = output_files_path+'query_res/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5976a558-8daa-4210-962c-07ad0d69a08a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee34caa7-56a4-44c1-8ee1-c516fa0055f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "stoplist = '../../IR_data/AP_DATA/stoplist.txt'\n",
    "with open(stoplist, 'r') as f:\n",
    "    stop_words = set(f.read().split())\n",
    "\n",
    "regex = r'[a-z0-9]+(?:\\.[a-z0-9]+)*'\n",
    "def tokenize(text):\n",
    "    tokens = re.findall(regex, text, re.IGNORECASE)\n",
    "    processed_tokens = []\n",
    "    for word in tokens: \n",
    "        word = word.strip().lower()\n",
    "        if word not in stop_words: \n",
    "            processed_tokens.append(word)\n",
    "    return processed_tokens\n",
    "def stem_text(tokens):\n",
    "    processed_tokens = []\n",
    "    for word in tokens: \n",
    "        w = ps.stem(word)\n",
    "        processed_tokens.append(w)\n",
    "    return processed_tokens\n",
    "def process_text(text):\n",
    "    processed_text = tokenize(text)\n",
    "    return processed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd950ebd-298d-4e90-973b-c7e51ba1ddf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84678\n"
     ]
    }
   ],
   "source": [
    "text_map_without_stemming = defaultdict(list)\n",
    "text_map_with_stemming = defaultdict(list)\n",
    "folder = '../../IR_data/AP_DATA/ap89_collection'\n",
    "\n",
    "\n",
    "for filename in os.listdir(folder):\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    with open(file_path, 'rb') as f:\n",
    "        content = f.read().decode('iso-8859-1')\n",
    "    doc_regex = r'<DOC>(.*?)</DOC>'\n",
    "    for doc in re.findall(doc_regex, content, re.S):\n",
    "        docno = re.search(r'<DOCNO>(.*?)</DOCNO>', doc).group(1).strip()      \n",
    "        for each in re.findall(r'<TEXT>(.*?)</TEXT>', doc, re.S):    \n",
    "            text_map_without_stemming[docno].extend(process_text(each))\n",
    "            text_map_with_stemming[docno].extend(stem_text(process_text(each)))\n",
    "                \n",
    "print(len(text_map_without_stemming))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50586ffc-3565-4f6f-88cc-e8d40ddbaec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_doc_ids(docs, out_file):\n",
    "    with open(out_file, 'w') as f:\n",
    "        for i, doc_id in enumerate(docs):\n",
    "            f.write(f\"{doc_id} {i+1}\\n\")\n",
    "            \n",
    "def load_doc_id_map(filename):\n",
    "    doc_name_id_map = {}\n",
    "    doc_id_name_map = {}\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            name, id = line.split()\n",
    "            doc_name_id_map[name] = int(id)\n",
    "            doc_id_name_map[int(id)] = name\n",
    "    return doc_name_id_map, doc_id_name_map\n",
    "\n",
    "# write_doc_ids(text_map_without_stemming.keys(), mid_output_files_path+\"doc_ids.txt\")\n",
    "doc_name_id_map, doc_id_name_map = load_doc_id_map(ids_output_files_path+\"doc_ids.txt\")\n",
    "\n",
    "# write_doc_ids(all_terms, ids_output_files_path+\"term_ids.txt\")\n",
    "term_word_id_map, term_id_word_map = load_doc_id_map(ids_output_files_path+\"term_ids.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0581bc3b-ab09-4f7c-aec2-510f2a9fa116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inverted_index(text_map):\n",
    "    inverted_index = defaultdict(lambda: defaultdict(list))\n",
    "    for doc_id, tokens in text_map.items():\n",
    "        for position, term in enumerate(tokens):\n",
    "            inverted_index[term][doc_name_id_map[doc_id]].append(position)\n",
    "    return inverted_index        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e44518e5-4e2c-4b6b-9dab-237c6725fe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inverted_index_without_stemming = dict(get_inverted_index(text_map_without_stemming))\n",
    "inverted_index_with_stemming = dict(get_inverted_index(text_map_with_stemming))\n",
    "all_terms = set(inverted_index_without_stemming.keys()).union(set(inverted_index_with_stemming.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1de0c001",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_doc_string(input_string):\n",
    "    output_string = re.sub(r'\\[|\\]|\\(', '', input_string)\n",
    "    output_string = re.sub(r',', '', output_string)\n",
    "    output_string = re.sub(r'\\)', ',', output_string)\n",
    "    return output_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28b04306-ba46-47ff-8f95-20d1d157cc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_batch(inverted_index, batch_num, output_path):\n",
    "    catalog_file = output_path + f'catalog{batch_num}.txt' \n",
    "    index_file = output_path + f'index{batch_num}.bin'\n",
    "    with open(index_file, 'wb') as index, open(catalog_file, 'w') as catalog:\n",
    "        offset = 0\n",
    "        for term in sorted(inverted_index.keys(), key=lambda x: term_word_id_map[x]):\n",
    "            doc_pos = process_doc_string(str(list(inverted_index[term].items())))\n",
    "            data = pickle.dumps(gzip.compress(str.encode(doc_pos)))\n",
    "            index.write(data)\n",
    "            size = len(data)\n",
    "            catalog.write(f\"{term_word_id_map[term]} {offset} {size}\\n\")\n",
    "            offset += size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03fb8977-4f68-4f83-9848-c73942f7f97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(file_keys, index):\n",
    "    temp_without_stemming_d = {}\n",
    "    temp_with_stemming_d = {}\n",
    "    \n",
    "    for k in file_keys:\n",
    "        temp_without_stemming_d[k] = text_map_without_stemming[k]\n",
    "        temp_with_stemming_d[k] = text_map_with_stemming[k]\n",
    "    inverted_index_without_stemming = dict(get_inverted_index(temp_without_stemming_d))\n",
    "    write_batch(inverted_index_without_stemming, index, output_files_path + 'without_stemming/')\n",
    "    \n",
    "    inverted_index_with_stemming = dict(get_inverted_index(temp_with_stemming_d))\n",
    "    write_batch(inverted_index_with_stemming, index, output_files_path + 'with_stemming/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66a693f7-b57a-46fa-9ee3-cac477f84ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Thread(Thread-4 (process_batch), started 18111033344)>\n"
     ]
    }
   ],
   "source": [
    "from threading import Thread\n",
    "keys = list(text_map_without_stemming.keys())\n",
    "num_files = math.ceil(len(keys) / 1000) # 85 files\n",
    "threads = []\n",
    "for i in range(num_files):\n",
    "    start_index = i*1000\n",
    "    end_index = min((i+1)*1000, len(keys))\n",
    "    file_keys = keys[start_index:end_index]\n",
    "    t = Thread(target=process_batch, args=(file_keys, i))\n",
    "    threads.append(t)\n",
    "    t.start()\n",
    "    \n",
    "for t in threads:\n",
    "    print(t)\n",
    "    t.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91369d36-80c2-4111-91c3-b1cc10c1be1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_catalogs_and_index(catalog1, catalog2, merged_catalog, index1, index2, merged_index):\n",
    "    cat1 = load_catalog(catalog1) \n",
    "    cat2 = load_catalog(catalog2)\n",
    "    ind1 = open(index1, 'rb')\n",
    "    ind2 = open(index2, 'rb')\n",
    "    merged_cat = open(merged_catalog, 'w')\n",
    "    merged_ind = open(merged_index, 'wb')\n",
    "    \n",
    "    n1, n2 = len(cat1), len(cat2)\n",
    "    i, j, offset = 0, 0, 0\n",
    "    while i < n1 and j < n2:\n",
    "        if cat1[i][0] < cat2[j][0]: \n",
    "            merged_cat.write(f\"{cat1[i][0]} {offset} {cat1[i][2]}\\n\")\n",
    "            ind1.seek(cat1[i][1])\n",
    "            data = gzip.decompress(pickle.loads(ind1.read(cat1[i][2]))).decode('ISO-8859-1')\n",
    "            merged_ind.write(pickle.dumps(gzip.compress(str.encode(data))))\n",
    "            offset += cat1[i][2]\n",
    "            i += 1\n",
    "        elif cat1[i][0] > cat2[j][0]:\n",
    "            merged_cat.write(f\"{cat2[j][0]} {offset} {cat2[j][2]}\\n\")\n",
    "            ind2.seek(cat2[j][1])\n",
    "            data = gzip.decompress(pickle.loads(ind2.read(cat2[j][2]))).decode('ISO-8859-1')\n",
    "            merged_ind.write(pickle.dumps(gzip.compress(str.encode(data))))\n",
    "            offset += cat2[j][2]\n",
    "            j += 1\n",
    "        else:\n",
    "            ind1.seek(cat1[i][1])\n",
    "            data1 = gzip.decompress(pickle.loads(ind1.read(cat1[i][2]))).decode('ISO-8859-1')\n",
    "            ind2.seek(cat2[j][1])\n",
    "            data2 = gzip.decompress(pickle.loads(ind2.read(cat2[j][2]))).decode('ISO-8859-1')\n",
    "            data = pickle.dumps(gzip.compress(str.encode(data1 + ' ' + data2)))\n",
    "            size = len(data)\n",
    "            merged_cat.write(f\"{cat1[i][0]} {offset} {size}\\n\")\n",
    "            merged_ind.write(data)\n",
    "            offset += size\n",
    "            i += 1\n",
    "            j += 1\n",
    "\n",
    "    while i < n1:\n",
    "        merged_cat.write(f\"{cat1[i][0]} {offset} {cat1[i][2]}\\n\")\n",
    "        ind1.seek(cat1[i][1])\n",
    "        data = gzip.decompress(pickle.loads(ind1.read(cat1[i][2]))).decode('ISO-8859-1')\n",
    "        merged_ind.write(pickle.dumps(gzip.compress(str.encode(data))))\n",
    "        offset += cat1[i][2]\n",
    "        i += 1\n",
    "\n",
    "    while j < n2:\n",
    "        merged_cat.write(f\"{cat2[j][0]} {offset} {cat2[j][2]}\\n\")\n",
    "        ind2.seek(cat2[j][1])\n",
    "        data = gzip.decompress(pickle.loads(ind2.read(cat2[j][2]))).decode('ISO-8859-1')\n",
    "        merged_ind.write(pickle.dumps(gzip.compress(str.encode(data))))\n",
    "        offset += cat2[j][2]\n",
    "        j += 1\n",
    "\n",
    "    merged_cat.close()\n",
    "    merged_ind.close()\n",
    "\n",
    "def load_catalog(filename):\n",
    "    with open (filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        catalog = [tuple(map(int, line.split())) for line in lines]\n",
    "    return catalog\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227d1eb9-66a5-4aa3-be94-c03a0ef4f50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def merge_all_files(file_path):\n",
    "    merged_file_path = file_path+'merged/'\n",
    "    for i in range(1, 85):\n",
    "        if i==1:\n",
    "            merge_catalogs_and_index(file_path+f'catalog{i-1}.txt', \n",
    "                                 file_path+f'catalog{i}.txt',\n",
    "                                 merged_file_path+f'catalog_merged_temp{i}.txt',\n",
    "                                 file_path+f'index{i-1}.bin',\n",
    "                                 file_path+f'index{i}.bin', \n",
    "                                 merged_file_path+f'index_merged_temp{i}.bin')\n",
    "        \n",
    "        elif i==84:\n",
    "            merge_catalogs_and_index(merged_file_path+f'catalog_merged_temp{i-1}.txt', \n",
    "                                file_path+f'catalog{i}.txt',\n",
    "                                merged_file_path+f'catalog_merged_final.txt',\n",
    "                                merged_file_path+f'index_merged_temp{i-1}.bin',\n",
    "                                file_path+f'index{i}.bin', \n",
    "                                merged_file_path+f'index_merged_final.bin')\n",
    "        else:\n",
    "            merge_catalogs_and_index(merged_file_path+f'catalog_merged_temp{i-1}.txt', \n",
    "                                file_path+f'catalog{i}.txt',\n",
    "                                merged_file_path+f'catalog_merged_temp{i}.txt',\n",
    "                                merged_file_path+f'index_merged_temp{i-1}.bin',\n",
    "                                file_path+f'index{i}.bin', \n",
    "                                merged_file_path+f'index_merged_temp{i}.bin')\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e960ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_all_files(with_stemming_output_fils_path)\n",
    "merge_all_files(without_stemming_output_files_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70d07e1-194e-4196-9a90-51aaa765a956",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_stop_words = stop_words.union(('document', 'noncommunist', 'locat', 'least', 'countri', 'second', 'unsubstanti', 'worldwid', 'exist', \n",
    "                               'product', 'preliminari', 'perpetr', 'aid', 'success', 'predict', 'describ', 'identifi', 'make', 'undesir',\n",
    "                               'level', 'determin', 'perform', 'platform', 'someth', 'side', 'effort', 'standard', 'motiv',\n",
    "                               'controversi', 'measur', 'tent', 'sign', 'individu', 'develop', 'nation', 'pend',\n",
    "                               'includ', 'result', 'anticip', 'support', 'ani', 'ha', 'directli', 'border' ,'area', 'base',\n",
    "                              'affair', 'ongo', 'method', 'sinc', 'system', 'candid', 'specifi', 'advanc', 'polit', 'attempt', 'asset'\n",
    "                              , 'organ','u s'))\n",
    "def query_stem_text_and_remove_stopwords(tokens):\n",
    "    processed_tokens = []\n",
    "    for word in tokens: \n",
    "        w = ps.stem(word)\n",
    "        w = w.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation))).strip()\n",
    "        if w!='' and w.lower() not in query_stop_words and w not in processed_tokens:\n",
    "            processed_tokens.append(w)\n",
    "    return ' '.join(processed_tokens)\n",
    "    \n",
    "def process_query(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    processed_tokens = query_stem_text_and_remove_stopwords(tokens)\n",
    "    return processed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f778a1e-aca6-4527-acfb-4a73fd638a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85 alleg corrupt public offici government jurisdict\n",
      "59 weather caus fatal\n",
      "56 prime lend rate\n",
      "71 incurs militari forc guerrilla\n",
      "64 hostage tak\n",
      "62 militari coup d etat\n",
      "93 rifl associ nra\n",
      "99 iran contra\n",
      "58 rail strike\n",
      "77 poach wildlif\n",
      "54 contract agreement reserv launch commerci satellit\n",
      "87 current crimin action offic fail financi institut\n",
      "94 crime comput\n",
      "100 non communist industri regul transfer high tech good dual us technolog\n",
      "89 invest opec downstream oper\n",
      "61 israel iran contra\n",
      "95 comput applic crime solv\n",
      "68 studi concern safeti manufactur employe instal worker fine diamet fiber insul\n",
      "57 mci bell\n",
      "97 instanc fiber optic technolog\n",
      "98 produc fiber optic equip\n",
      "60 salari incent pay contrast sole basi senior longev\n",
      "80 1988 presidenti\n",
      "63 machin translat\n",
      "91 acquisit armi weapon\n"
     ]
    }
   ],
   "source": [
    "query_file = '../../IR_data/AP_DATA/query_desc.51-100.short.txt'\n",
    "query_map = {}\n",
    "with open(query_file, 'r') as f: \n",
    "    query_content = f.read().split('\\n')\n",
    "for line in query_content:\n",
    "    dot_index = line.index('.')\n",
    "    query_map[line[:dot_index]] = process_query(line[dot_index+1:].strip())\n",
    "for k,v in query_map.items():\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00aada74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_term_doc_frequencies(index_file_path, catalog_file_path):\n",
    "    term_frequencies = {}\n",
    "    doc_frequencies = {}\n",
    "    \n",
    "    with open(index_file_path, 'rb') as ind, open(catalog_file_path, 'r') as cat:\n",
    "        catalog = cat.readlines()\n",
    "        for line in catalog:\n",
    "            term_id, offset, size = map(int, line.split())\n",
    "            term = term_id_word_map[term_id]\n",
    "            ind.seek(offset)\n",
    "            term_info = gzip.decompress(pickle.loads(ind.read(size))).decode('ISO-8859-1')\n",
    "            term_vectors = term_info.split(',')[:-1]\n",
    "            df = len(term_vectors)\n",
    "            for each_doc in term_vectors:\n",
    "                info = each_doc.split()\n",
    "                doc_id, pos = doc_id_name_map[int(info[0])], list(map(int, info[1:]))\n",
    "                if doc_id not in term_frequencies:\n",
    "                    term_frequencies[doc_id] = {}\n",
    "                    doc_frequencies[doc_id] = {}\n",
    "                term_frequencies[doc_id][term] = len(pos)\n",
    "                doc_frequencies[doc_id][term] = df\n",
    "                \n",
    "    return term_frequencies, doc_frequencies\n",
    "\n",
    "term_frequencies_with_stemming, doc_frequencies_with_stemming = get_term_doc_frequencies( \n",
    "    merged_files_path+'compressed/index_merged_final_with_stemming.bin',\n",
    "    merged_files_path+'compressed/catalog_merged_final_with_stemming.txt')\n",
    "term_frequencies_without_stemming, doc_frequencies_without_stemming = get_term_doc_frequencies(\n",
    "    merged_files_path+'compressed/index_merged_final_without_stemming.bin',\n",
    "    merged_files_path+'compressed/catalog_merged_final_without_stemming.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c714c503-9c79-4dac-bd2b-f684088104f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236733"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_okapi_TF(tf, doc_length, avg_len_d):\n",
    "    return tf / (tf + 0.5 + 1.5 * (doc_length / avg_len_d))\n",
    "\n",
    "def calculate_tfidf(tf, df, doc_length, num_doc, avg_len_d):\n",
    "    term_f = (tf / (tf + 0.5 + 1.5 * (doc_length / avg_len_d)))\n",
    "    idf = math.log(num_doc / df) if df else 0\n",
    "    return  term_f * idf\n",
    "\n",
    "def compute_okapi_bm25(tf, df, doc_length, avg_len_d, num_doc):\n",
    "    k1, k2, b = 1.2, 1,0.75\n",
    "    return math.log((num_doc + 0.5) / (df + 0.5)) * ((tf + k1 * tf) / (tf + k1 * ((1 - b) + b * (doc_length / avg_len_d)))) * ((tf + k2 * tf)/(tf + k2))\n",
    "\n",
    "def compute_unigram_lml(tf, doc_length, num_unique_words):\n",
    "    if tf!=0:\n",
    "        return math.log((tf + 1) / (doc_length + num_unique_words))\n",
    "    else: \n",
    "        return -1000\n",
    "    \n",
    "def compute_unigram_lmjm(tf, ttf, doc_length, num_unique_words, lambda_const):\n",
    "    if tf!=0:\n",
    "        return math.log(lambda_const * (tf / doc_length) + (1 - lambda_const) * (ttf / num_unique_words))\n",
    "    else: \n",
    "        return -1000\n",
    "    \n",
    "num_unique_words = len(all_terms)\n",
    "num_unique_words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83e4906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_len(term_frequencies):\n",
    "    avg_len_d = 0\n",
    "    for k,v in term_frequencies.items():\n",
    "        avg_len_d+=len(v)\n",
    "    avg_len_d = avg_len_d//len(term_frequencies)\n",
    "    return avg_len_d\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a57a195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_output_scores(scores):\n",
    "    output=''\n",
    "    for query_id, doc_ids in scores.items():\n",
    "        for rank, (doc_id, score) in enumerate(doc_ids):\n",
    "            output += query_id + ' Q0 ' + str(doc_id) + ' ' + str(rank+1) + ' ' + str(score) + ' Exp\\n'\n",
    "    return output\n",
    "\n",
    "\n",
    "def output_txt(filename, string, save_path):\n",
    "    with open(save_path+filename+'.txt', 'w') as f: \n",
    "        f.write(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b8e1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_term_positions(query_map, index_file_path, catalog_file_path):\n",
    "    term_positions = {}\n",
    "    catalog = {}\n",
    "    with open(index_file_path, 'rb') as ind, open(catalog_file_path, 'r') as cat:\n",
    "        cat = cat.readlines()\n",
    "        for line in cat:\n",
    "            term_id, offset, size = map(int, line.split())\n",
    "            catalog[term_id_word_map[term_id]] = [offset, size]\n",
    "        \n",
    "        for query_id, query in query_map.items():\n",
    "            for term in query.split():\n",
    "                if term not in term_positions:\n",
    "                    if term in catalog:\n",
    "                        ind.seek(catalog[term][0])\n",
    "                        term_info = gzip.decompress(pickle.loads(ind.read(catalog[term][1]))).decode('ISO-8859-1')\n",
    "                        postings = term_info.split(',')[:-1]\n",
    "                        for each_doc in postings:\n",
    "                            info = each_doc.split()\n",
    "                            document_id, document_positions =  doc_id_name_map[int(info[0])], list(map(int, info[1:])) \n",
    "                            if term not in term_positions:\n",
    "                                term_positions[term] = {}\n",
    "                            term_positions[term][document_id] = document_positions\n",
    "    return term_positions\n",
    "\n",
    "term_positions = get_term_positions(query_map, merged_files_path+'compressed/index_merged_final_with_stemming.bin', merged_files_path+'compressed/catalog_merged_final_with_stemming.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423364fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getProximityScore(list_of_lists):\n",
    "    window = []\n",
    "    for l in list_of_lists:\n",
    "        window.append(l[0])\n",
    "    score = math.inf\n",
    "\n",
    "    while True:\n",
    "        min_idx = window.index(min(window))\n",
    "        min_val = window[min_idx]\n",
    "        next_idx = list_of_lists[min_idx].index(min_val) + 1\n",
    "        if next_idx < len(list_of_lists[min_idx]):\n",
    "            window[min_idx] = list_of_lists[min_idx][next_idx] \n",
    "            score = min(score, max(window) - min(window))\n",
    "        else:\n",
    "            break\n",
    "    s = 40\n",
    "    if score<=s:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a70e15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_stop_words = stop_words.union(('document', 'noncommunist', 'locat', 'least', 'countri', 'second', 'unsubstanti', 'worldwid', 'exist', \n",
    "                               'product', 'preliminari', 'perpetr', 'aid', 'success', 'predict', 'describ', 'identifi', 'make', 'undesir',\n",
    "                               'level', 'determin', 'perform', 'platform', 'someth', 'side', 'effort', 'standard', 'motiv',\n",
    "                               'controversi', 'measur', 'tent', 'sign', 'individu', 'develop', 'nation', 'pend',\n",
    "                               'includ', 'result', 'anticip', 'support', 'ani', 'ha', 'directli', 'border' ,'area', 'base',\n",
    "                              'affair', 'ongo', 'method', 'sinc', 'system', 'candid', 'specifi', 'advanc', 'polit', 'attempt', 'asset'\n",
    "                              , 'organ','u s', 'prediction', 'location', 'country', 'controversial',\n",
    "                                'measure', 'signs', 'individual', 'developed', 'pending', 'directli', 'politically',\n",
    "                                  'attempted', 'assets', 'organization', 'successful', 'development', 'ongoing', 'preliminary', \n",
    "                                  'perpetrated', 'states', 'nations', 'products', 'unsubstantiated', \n",
    "                                  'prganizayions', 'performance', 'candidate'))\n",
    "def query_stem_text_and_remove_stopwords(tokens, stem):\n",
    "    processed_tokens = []\n",
    "    for w in tokens: \n",
    "        if stem:\n",
    "            w = ps.stem(w)\n",
    "        w = w.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation))).strip()\n",
    "        if w!='' and w.lower() not in query_stop_words and w not in processed_tokens:\n",
    "            processed_tokens.append(w.lower())\n",
    "    return ' '.join(processed_tokens)\n",
    "    \n",
    "def process_query(text, stem):\n",
    "    tokens = word_tokenize(text)\n",
    "    processed_tokens = query_stem_text_and_remove_stopwords(tokens, stem=stem)\n",
    "    return processed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfcbbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_file = '../../IR_data/AP_DATA/query_desc.51-100.short.txt'\n",
    "query_map = {}\n",
    "query_map_unstemmed = {}\n",
    "with open(query_file, 'r') as f: \n",
    "    query_content = f.read().split('\\n')\n",
    "for line in query_content:\n",
    "    dot_index = line.index('.')\n",
    "    query_map[line[:dot_index]] = process_query(line[dot_index+1:].strip(), True)\n",
    "    query_map_unstemmed[line[:dot_index]] = process_query(line[dot_index+1:].strip(), False)\n",
    "for k,v in query_map.items():\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7231628a-f838-4506-80e0-c4ab2cc7daae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_documents(query_map, term_frequencies, doc_frequencies, avg_len_d, model):\n",
    "    model_scores = defaultdict(dict)\n",
    "    for doc_id, doc in term_frequencies.items():\n",
    "        len_d = len(doc)\n",
    "        num_doc = len(term_frequencies.keys())\n",
    "        for query_id, query in query_map.items():\n",
    "            score = 0\n",
    "            term_windows = {}\n",
    "            for term in query.split():\n",
    "                tf, df = 0, 0 \n",
    "                if term in doc: \n",
    "                    tf = doc[term]\n",
    "                    df = doc_frequencies[doc_id][term]\n",
    "                if model=='okapi_tf':\n",
    "                    score += calculate_okapi_TF(tf, len_d, avg_len_d)\n",
    "                elif model=='tfidf':\n",
    "                    score += calculate_tfidf(tf, df, len_d, num_doc, avg_len_d)\n",
    "                elif model=='okapi_bm25':\n",
    "                    score += compute_okapi_bm25(tf, df, len_d, avg_len_d, num_doc)\n",
    "                elif model=='unigram_lml':\n",
    "                    score += compute_unigram_lml(tf, len_d, num_unique_words)\n",
    "                elif model=='proximity':\n",
    "                    score+= compute_okapi_bm25(tf, df, len_d, avg_len_d, num_doc)\n",
    "                    if term in term_positions and doc_id in term_positions[term]:\n",
    "                        term_windows[term]= term_positions[term][doc_id]\n",
    "                    if len(term_windows)>2:\n",
    "                        if getProximityScore(list(term_windows.values())):\n",
    "                            score+= calculate_tfidf(tf, df, len_d, num_doc, avg_len_d) \n",
    "\n",
    "                        \n",
    "                    \n",
    "            if score!=0:\n",
    "                if doc_id not in model_scores[query_id]:\n",
    "                    model_scores[query_id][doc_id] = score\n",
    "                else:\n",
    "                    model_scores[query_id][doc_id] += score\n",
    "    ranked_documents = {}\n",
    "    for query_id, doc_scores in model_scores.items():\n",
    "        ranked_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)[:1000]\n",
    "        ranked_documents[query_id] = ranked_docs\n",
    "    ranked_documents\n",
    "    return ranked_documents\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5eab9d-d9d4-42c4-a6a1-52fb64d8b4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(query_map, term_frequencies, doc_frequencies, avg_len_d, save_location):\n",
    "    filename = 'okapi_tf'\n",
    "    okapi_tf_ranked_documents = rank_documents(query_map, term_frequencies , doc_frequencies, avg_len_d, model='okapi_tf')\n",
    "    output = process_output_scores(okapi_tf_ranked_documents)\n",
    "    output_txt(filename, output, save_location)\n",
    "\n",
    "    filename = 'tfidf'\n",
    "    tfidf_ranked_documents = rank_documents(query_map, term_frequencies , doc_frequencies, avg_len_d, model='tfidf')\n",
    "    output = process_output_scores(tfidf_ranked_documents)\n",
    "    output_txt(filename, output, save_location)\n",
    "\n",
    "    filename = 'okapi_bm25'\n",
    "    okapi_bm25_ranked_documents = rank_documents(query_map, term_frequencies, doc_frequencies, avg_len_d, model='okapi_bm25')\n",
    "    output = process_output_scores(okapi_bm25_ranked_documents)\n",
    "    output_txt(filename, output, save_location)\n",
    "\n",
    "    filename = 'unigram_lml'\n",
    "    unigram_lml_ranked_documents = rank_documents(query_map, term_frequencies, doc_frequencies, avg_len_d, model='unigram_lml')\n",
    "    output = process_output_scores(unigram_lml_ranked_documents)\n",
    "    output_txt(filename, output, save_location)\n",
    "    filename = 'proximity'\n",
    "    unigram_lml_ranked_documents = rank_documents(query_map, term_frequencies, doc_frequencies, avg_len_d, model='proximity')\n",
    "    output = process_output_scores(unigram_lml_ranked_documents)\n",
    "    output_txt(filename, output, save_location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a011b97-2258-4961-a3f5-b0a0b306844b",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_scores(query_map, term_frequencies_with_stemming, doc_frequencies_with_stemming, get_avg_len(term_frequencies_with_stemming), output_query_res+'with_stemming/')\n",
    "get_scores(query_map_unstemmed, term_frequencies_without_stemming, doc_frequencies_without_stemming, get_avg_len(term_frequencies_without_stemming), output_query_res+'without_stemming/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3788bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64bbd14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
